{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "x00t_uJCEbeb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "#@title Setup Environment\n",
    "# Install the latest Tensorflow version.\n",
    "!pip install -q tensorflow-text\n",
    "!pip install annoy\n",
    "!pip install simpleneighbors[annoy]\n",
    "# !pip install -q nltk\n",
    "# !pip install -q tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import simpleneighbors\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import HTML, display\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import tensorflow.compat.v2 as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_text import SentencepieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "DmeFAuVsyWxg",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package punkt to /home/sandipan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "questions = []\n",
    "# Create the global index object\n",
    "index = simpleneighbors.SimpleNeighbors(512, metric='angular')\n",
    "\n",
    "def load_data(path='../data/faq.csv', sep=\"##,##\"):\n",
    "    print('Loading data....')\n",
    "    data = pd.read_csv(path,sep='##,##')\n",
    "    print('...... done')\n",
    "    return data\n",
    "\n",
    "def extract_sentences_from_answer(df):\n",
    "  all_sentences = []\n",
    "  for index, row in df.iterrows():\n",
    "      # nltk.tokenize.sent_tokenize is a unsupervised algo\n",
    "      sentences = nltk.tokenize.sent_tokenize(row['answer'])    \n",
    "      sentences = sentences + nltk.tokenize.sent_tokenize(row['question'])\n",
    "      all_sentences.extend(zip(sentences, [row['answer']] * len(sentences)))\n",
    "  return list(set(all_sentences)) # remove duplicates\n",
    "\n",
    "def extract_questions(df):  \n",
    "  for index, row in df.iterrows():\n",
    "    questions.append((row['question'], row['answer']))   \n",
    "  return list(set(questions))\n",
    "        \n",
    "\n",
    "def get_nearest(query_text):\n",
    "  query_embedding = model.signatures['question_encoder'](tf.constant([query_text]))['outputs'][0]\n",
    "  search_results = index.nearest(query_embedding, n=num_results)\n",
    "  return search_results\n",
    "\n",
    "def load_USE_model(path='/home/sandipan/projects/model/'):  \n",
    "  print('Loading Universal Sentence Encoder from', path)\n",
    "  m = hub.load(path)\n",
    "  print('Universal Sentence Encoder model from in mem', m)\n",
    "  return m\n",
    "\n",
    "def build_search_index(data, model, batch_size = 10, index_path='/home/sandipan/projects/index/faq.ann'):\n",
    "  sentences = extract_sentences_from_answer(data)\n",
    "  encodings = model.signatures['response_encoder'](\n",
    "    input=tf.constant([sentences[0][0]]),\n",
    "    context=tf.constant([sentences[0][1]]))\n",
    "  print('Computing embeddings for %s sentences' % len(sentences))\n",
    "  slices = zip(*(iter(sentences),) * batch_size)\n",
    "  num_batches = int(len(sentences) / batch_size)\n",
    "  print('Batch wise index add...')\n",
    "  for s in tqdm(slices, total=num_batches):\n",
    "    response_batch = list([r for r, c in s])\n",
    "    context_batch = list([c for r, c in s])\n",
    "    encodings = model.signatures['response_encoder'](\n",
    "                  input=tf.constant(response_batch),\n",
    "                  context=tf.constant(context_batch)\n",
    "                )    \n",
    "    for batch_index, batch in enumerate(response_batch):\n",
    "      index.add_one(batch, encodings['outputs'][batch_index])\n",
    "\n",
    "  print('Building Index...')\n",
    "  index.build()\n",
    "  print('Saving Index...')\n",
    "  index.save(index_path)\n",
    "  return dict(sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Loading data....\n...... done\n                   question                                             answer\n0            What is NEFT ?  National Electronic Funds Transfer or NEFT is ...\n1  Where is mount everest ?  Mount Everest is the highest of the Himalayan ...\nLoading Universal Sentence Encoder from /home/sandipan/projects/model/\nUniversal Sentence Encoder model from in mem <tensorflow.python.saved_model.load.Loader._recreate_base_user_object.<locals>._UserObject object at 0x7f36bf4d2410>\nComputing embeddings for 39 sentences\nBatch wise index add...\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f53925c9321e42c1a17452e541721835"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nBuilding Index...\nSaving Index...\n"
    }
   ],
   "source": [
    "data = load_data('../data/faq.csv')\n",
    "print(data.head(2))\n",
    "model = load_USE_model('/home/sandipan/projects/model/')\n",
    "# extract_questions(data)\n",
    "sentence_dict = build_search_index(data = data,model = model, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "J0xTw2w3UViK",
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'The system monitors trends in member access and activity within Digital Banking. We may prompt for additional authentication based on those trends.For example, you might get challenged if you:Clear your browser cookies.Use a new device or computer.Log in from a new location.Please note that this is not a comprehensive list of things that could prompt for challenge questions. To protect member security, Coastal is not able to share details surrounding the reasons for receiving additional authentication challenges.'}\n"
    }
   ],
   "source": [
    "#@title Retrieve nearest neighbors for a random question from SQuAD\n",
    "num_results = 10 #@param {type:\"slider\", min:5, max:40, step:1}\n",
    "\n",
    "s= get_nearest('protect security of member')\n",
    "ans = set()\n",
    "for i in s:\n",
    "    ans.add(sentence_dict[s[0]])\n",
    "\n",
    "print(ans)    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VFMCdVJIIraw"
   ],
   "name": "Universal Encoder Q&A Model Retrieval Demo",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}